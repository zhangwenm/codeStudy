## Zookeeper 

#### 应用场景
- 分布式配置中心
- 分布式注册中心
- 分布式锁
- 集群选举
- 发布订阅
####核心概念
- 分布式协调框架
- 基于内存的数据库
- 维护了一个类似文件系统的数据结构
#### 节点类型
- 持久化节点
- 持久化顺序节点
- 临时节点（生命周期与session绑定，30秒超时。可以在客户端配置），不能有子节点
- 临时顺序节点
- container节点，没有子节点60秒后删除
- ttl节点，可以设置超时，后台轮询去检查删除
#### 监听
- 监听数据
- 监听节点
- 递归监听目录
#### 数据持久化
- 事务日志（磁盘空间预分配），数据更全
- 数据快照，快速恢复。某一时刻内存中的全量数据
#### 删除
- 删除的时候可以指定版本，乐观锁
- 指定-1会匹配所有版本
#### curator zookeeper客户端
- 递归创建节点
- protection防止重试创建僵尸节点
#### 集群模式
- leader 负责处理读写数据。先写事务日志，再发数据给其他follower，只有过半的follower也写事务日志成功后，leader才会把数据写内存
- follower 负责读数据，如果leader失效，可以参与选举投票，有机会成为新的leader。如果收到写请求会转发到leader
- Observre 负责读数据，减轻服务端写数据的压力
#### 分布式锁
- 非公平锁。连接较多时，压力较大
- 公平锁 顺序节点 curator
  - 父节点为容器节点，子节点临时顺序节点
- redis和zookeeper区别
  - zookeeper可靠性较高，但是写的性能略差，需要半数以上原则
  - redis数据可能丢失，性能较好
- 共享锁(共享锁)
#### 选举 
- ![](./pic/zk_sel.png)
- myid zxid 选zxid大的，zxid相同比较myid。节点数已配置文件里配置的为准

- zk的选举底层可以分为选举应用层和消息传输层，应用层有自己的队列收、发选票，传输层也设计了自己的队列，并按发送的机器分了队列，避免每台机器  
发送消息时相互影响，比如某台机器如果出问题发送不成功不会影响对正常机器的发送  
- ![](./pic/zk_queue.png "zk队列")  
#### ZAB协议
- zk原子广播协议
- zk为分布式服务提供高效且可靠的分布式协调服务。在解决分布式一致性方面，zk没有使用paxos，而是采用了ZAB协议，paxos的一种简化版算法
- 支持崩溃恢复和原子广播
- zk实现了一种主备架构来保持集群各副本之间数据的一致性
- ![](./pic/zk_broadcast.png "zk广播")  
- 消息广播
  - 类似二阶段提交的原子广播协议。
  - 客户端发送的写请求全部交由leader处理，leader将请求封装成proposal，将其发送给所有的follower，由半数以上的follower反馈成功则进行commit  
  操作
  - 每个proposal会有一个全局递增唯一的id，成为事务id（zxid），通过消息队列来保证顺序处理
  - leader和follower之间还有一个消息队列，来解耦，接触同步阻塞
  - 为保证集群所有进程都能够有序的顺序执行，只能是leader来处理写请求，follower接到写请求会转发给leader。follower只处理读请求
  - ZAB协议规定，如果一个事务在一台机器上执行成功，那么应该在所有机器上都执行成功，哪怕机器出现故障
  - ![](./pic/zk_broadcast_process.png "zk广播流程") 
#### 崩溃恢复
- leader复制数据给所有follower，还没来得及收到ack，挂掉 
- leader收到ack发送部分commit之后挂掉
- 为此ZAB协议定义了两个原则
  - 确保丢弃那些只在leader提出/复制，没有commit的事务
  - 确保那些已经在leader提交的的事务，被所有follower提交
- 确保提交已经被leader提交的事务，丢弃已经被跳过的事务
- 选举算法思想：每次选举出来的leader都拥有所有机器zxid最大的事务，这样就能保证leader具有所有已经提交的事务
  - 可以省去leader检查事务的提交和丢弃工作这一步骤
- leader服务器处理或丢弃事务都是依赖着zxid的
  - zxid是一个64位数字，其中低32位可以看做一个简单的递增计数器，针对客户端的每一个事务请求，leader都会产生一个新的proposal并将该  
  计数器+1
  - 高32位则代表了leader上本地最大事务proposal的epoch值（leader选举周期），当一轮选举结束后，会对这个值+1，事务id从0开始 
  - 高32位代表了leader的唯一性，低32位代表每代事务的唯一性
    - ![](./pic/zk_zxid.png "zxid") 
#### 选举leader
- 创建当前节点对象，并设置选举类型
- 加载已有数据
- 启动节点，绑定2181 端口用于节点间通信
- 构建当前节点选票
- 创建选举算法
  - 启动选举监听器并监听相应端口等待接收消息
  - 处理连接请求
  - 读取到链接的机器id
    - 会进行判断只允许id大的链接向机器id小的发起连接，防止重复建立socket
    - 建立连接并创建选票接收线程，
      - 启动并将选票放入传输层接收队列
      - 创建选票发送器线程并与目标机器sid绑定
      - 为每个目标机器创建单独的缓存队列
        - 启动选票发送线程并从目标机器对应缓存队列中取数据进行发送
  - 创建选举
    - 初始化业务层收发队列
  - 启动相关线程
    - 从应用层发送队列拉取选票进行发送
      - 如果sid与myid相等，将选票放入自己的传输层接收队列
      - 否则放入传输层目标机器对应的队列并建联
    - 从传输层接收队列拉取选票
      - 如果当前节点还在选举则将选票放入应用层接收队列
      - 选票来源机器也在选举并且周期小于自己，则将自己的选票反发
      - 当前节点不在选举且来源及其还在选举，则将当前机器认为的leader反发
  - 启动选举线程
    - 当前节点是looking状态
      - 选举周期增加
      - 初始化选自己选票放入应用层发送队列
      - 循环选举
        - 从应用层接收队列拉取数据
          - 拉取为空则循环与及集群其他节点建立连接
          - 不为空且为following/leading，说明别的节点已经选举出leader，如果本机投的leader是自己责自己就是leader，否则就是follower或者observer
          - 不为空且是looking
            - 选举周期小于自己直接忽略
            - 相等则将收到的和自己投的作对比
            - 大于自己说明自己落后
              - 将投自己的和该选票对比，要不自己获胜，要不别的选票获胜
          - 经过判断后更新选票信息
          - 遍历参加选举的sid，构建对应的通知选票加入到应用层发送队列
          - 将收到的选票加入选票箱
          - 过半选举判断选出leader
    - 当前节点是following则与leader建立连接并同步数据
      - 如果leader故障则将自己设置为looking进入下一次循环选举
    - 当前节点为leader
      - 监听follower的socket
      - 同步数据给从节点
      - 死循环定时与follower的发送ping消息保持长链接