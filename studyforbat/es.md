## Elasticsearch

#### 分布式搜索引擎，基于lucene框架 
- lucene
  - 只能在java中使用
  - 代码繁杂
  - 不支持集群
  - 索引和应用在同一机器
  - ES 配合三方分词器（最粗粒度，最细粒度）
    - INDEX(database数据库)，相似文档的集合
    - mapping（文档结构）
    - type（表）
    - Documnet（行）
    - field字段
- 倒排索引
  - 存储的的时候进行关键词切分，建立关键词与文档的对应关系表，称之为倒排索引（反向索引）
  - 分词（位置，出现次数）
  - 排序
- DSL查询
  - query DSL
  - filter DSL
- query DSL 结果不会放进缓存
  - term 不会进行分词
  - match 模糊匹配 会对关键字分词
- 文档映射
  - 动态映射：不需要mapping，自动根据字段识别类型
  - 静态映射：先定义好字段类型、分词器等
  - index：属性用来配置是否开启分词建立对应关系
  - store：是否存储文档数据
  - String 包含text keyword
    - text会进行分词，不能排序和聚合
    - keyword不会进分词
  - 修改静态映射：命令行将原索引数据导入新索引，重命名新索引
- 分页
  - from + size 浅分页
    - "浅"分页可以理解为简单意义上的分页。它的原理很简单，就是查询前20条数据，然后截断前10条，只返回10-20的数据。  
    这样其实白白浪费了前10条的查询。
    - 因为es是基于分片的，假设有5个分片，from=100，size=10。则会根据排序规则从5个分片中各取回110条数据数据，  
    然后汇总成550条数据后选择最后面的10条数据。
  - scroll 深分页
    - scroll 类似于sql中的cursor，使用scroll，每次只能获取一页的内容，然后会返回一个scroll_id。根据返回的这  
    个scroll_id可以不断地获取下一页的内容，所以scroll并不适用于有跳页的情景。
    - scroll=5m表示设置scroll_id保留5分钟可用。
      使用scroll必须要将from设置为0。
      size决定后面每次调用_search搜索返回的数量
    - scroll删除
      - 根据官方文档的说法，scroll的搜索上下文会在scroll的保留时间截止后自动清除，但是我们知道scroll是非常消耗资源的，  
      所以一个建议就是当不需要了scroll数据的时候，尽可能快的把scroll_id显式删除掉。
  - search_after 深分页
    - scroll 的方式，官方的建议不用于实时的请求（一般用于数据导出），因为每一个 scroll_id 不仅会占用大量的资源，而且会生  
    成历史快照，对于数据的变更不会反映到快照上。
    - search_after 分页的方式是根据上一页的最后一条数据来确定下一页的位置，同时在分页请求的过程中，如果有索引数据的增删改查，  
    这些变更也会实时的反映到游标上。但是需要注意，因为每一页的数据依赖于上一页最后一条数据，所以无法跳页请求。
    - 为了找到每一页最后一条数据，每个文档必须有一个全局唯一值，官方推荐使用 _uid 作为全局唯一值，其实使用业务层的 id 也可以。
    - 使用search_after必须要设置from=0。
      这里我使用timestamp和_id作为唯一值排序。
      我们在返回的最后一条数据里拿到sort属性的值传入到search_after。
#### 架构
- master，集群中只有一个master
  - 管理索引、分配分片
  - 负责切换 primary shard 和 replica shard 身份等
  - 维护元数据
  - 管理集群节点状态
  - 不负责写入和查询
- DataNode
  - 数据写入、检索，内存要大一些
  - 分片、副本。高版本默认一个
- 一个索引可以分为多个分片，每个分片又可以设置多个副本
  - 分片
    - 每个分片都有主分片
    - 主分片和从分片不在同一节点
    - 分片允许水平分割/扩展内容容量
    - 允许在分片上做分布式、并行的操作，提高吞吐率、
    - 不能改变分片数量
  - 副本
    - 容灾
    - 扩展搜索/吞吐量，搜索可以再所有的副本上并行进行
    - 可以动态的改变副本数量
- 乐观并发控制
  - 高版本用的seqno控制，低版本用version控制
- 写入流程
  - 选择任意一个DataNode发送请求，该节点成为协调节点
  - 计算得到要写入的分片 shard = hash(routing) % number_of_primary_shards
    routing 是一个可变值，默认是文档的 _id
  - 协调节点将请求路由到主分片所在的节点，处理写请求写入索引库并同步到其他副本
  - 主分片和从分片都处理好文档，返回client
    ![](/studyforbat/pic/es_write.png)
- 检索原理
  - client发起请求，某个DataNode收到请求成为协调节点
  - 协调节点将请求广播到数据节点，这些数据节点的分片会处理查询请求
  - 每个分片进行数据查询，将查询到符合条件的数据放入一个优先队列，并将这些数据的文档ID、节点信息、分片信息  
  返回个协调节点
  - 协调节点将结果汇总、全局排序
  - 协调节点向包含这些文档ID的分片发送请求，收到数据后返回给客户端
    ![](/studyforbat/pic/es_read.png)
- es准时数据库
  - 当数据写入到es分片时，会先写入到内存中，然后通过内存的buffer生成一个segment并刷到文件系统缓存中  
  之后文件才能被检索，默认每秒刷一次
  - 写入内存的同时，也会记录translog日志，在refresh期间出现异常会根据translog文件进行恢复
  - 等到segment数据都刷到磁盘清空translog日志
  - 默认30分钟将文件系统缓存刷盘
  - segment太多时会进行合并生成大的segment，减少索引查询的I/O开销，真正删除之前执行过delete操作的数据
- 不同节点介紹
  - 当节点node.master：false node.data: false时，该节点只能作为路由节点，用来协调主节点和数据节点，路由请求
  - 数据节点主要是存储索引数据的节点，并对文档进行操作，对cpu、内存、IO等要求较高
- 生产设置建议
  - 设置三台或以上的主节点，维护整个集群的状态。再根据数据量设置一批数据节点，负责存储数据。如果用户请求比较频繁的话  
  ，数据节点压力比较大，可以适当设置一些client节点，负责负载均衡请求转发等
#### 选举
- 正常情况下，集群中只有一个Leader，其他节点全是Follower。Follower 都是被动接收请求，从不主动发送任何请求。Candidate   
候选人，候补者；应试者  是从Follower到Leader的中间状态。
Raft中引入任期(term) 的概念，每个term内最多只有一个Leader。term 在Raft算法中充当逻辑时钟的作用。服务器之间通信的时候会携带这个term，  
如果节点发现消息中的term小于自己的term，则拒绝这个消息；如果大于本节点的term，则更新自己的term。如果一个Candidate或者Leader   
发现自己的任期过期了，它会立即回到Follower状态。
Raft选举流程为：
- 增加当前节点本地的current term，切换到Candidate状态；
- 当前节点投自己一票，并且并行给其他节点发送RequestVote RPC (让大家投他) ；
  - 然后等待其他节点的响应，会有如下三种结果：
  - 如果接收到大多数服务器的选票，那么就变成Leader。成为Leader后，向其他节点发送心跳消息来确定自己的地位并阻止新的选举。
  - 如果收到了别人的投票请求，且别人的term比自己的大，那么候选者退化为Follower；
  - 如果选举过程超时，再次发起一轮选举；
- ES实现Raft算法选主流程
  - ES实现中，候选人不先投自己，而是直接并行发起RequestVote，这相当于候选人有投票给其他候选人的机会。  
  这样的好处是可以在一定程度上避免3个节点同时成为候选人时，都投自己，无法成功选主的情况。

- ES不限制每个节点在某个term上只能投一票， 节点可以投多票，这样会产生选出多个主的情况：
  - Node2被选为主，收到的投票为：Node2、 Node3；
  - Node3被选为主，收到的投票为：Node3、 Node1；
  - 对于这种情况，ES的处理是让最后当选的Leader成功，作为Leader。如果收到RequestVote请求，  
  他会无条件退出Leader状态。在本例中，Node2先被选为主，随后他收到Node3的RequestVote请求，  
  那么他退出Leader状态，切换为CANDIDATE，并同意向发起RequestVote候选人投票。因此最终Node3成功当选为Leader。 
#### 动态维护参选节点列表
- 在此之前，我们讨论的前提是在集群节点数量不变的情况下，现在考虑下集群扩容、缩容、节点临时或永久离线时是如何处理的。  
- 在7.x之前的版本中，用户需要手工配置minimum_master_nodes, 来明确告诉集群过半节点数应该是多少，并在集群扩缩容时调整他。现在，集群可以自行维护。
- 在取消了discovery.zen.minimum_master_nodes 配置后，现在的做法不再记录“quorum”法定数量的具体数值，取而代之的是记录一个节点列表，  
这个列表中保存所有具备master资格的节点(有些情况下不是这样，例如集群原本只有1个节点，当增加到2个的时候，这个列表维持不变，因为如果变成2，  
当集群任意节点离线，都会导致无法选主。这时如果再增加一个节点，集群变成3个，这个列表中就会更新为3个节点)，称为VotingConfiguration，  
他会持久化到集群状态中。
- 在节点加入或离开集群之后，Elasticsearch 会自动对VotingConfiguration 做出相应的更改，以确保集群具有尽可能高的弹性。  
在从集群中删除更多节点之前，等待这个调整完成是很重要的。你不能一次性停止半数或更多的节点。(感觉大面积缩容时候这个操作就比较感人了，一部分一部分缩)。  
默认情况下，ES自动维护VotingConfiguration。有新节点加入的时候比较好办，但是当有节点离开的时候，他可能是暂时的重启，也可能是永久下线。  
你也可以人工维护VotingConfiguration，配置项为：cluster.auto_shrink_voting_configuration，当你选择人工维护时，有节点永久下线，  
需要通过voting exclusions API将节点排除出去。如果使用默认的自动维护VotingConfiguration，也可以使用voting exclusions API来排除  
节点，例如一次性下线半数以上的节点。
- 如果在维护VotingConfiguration时发现节点数量为偶数，ES会将其中一个排除在外，保证VotingConfiguration是奇数。因为当是偶数的情况下，  
网络分区将集群划分为大小相等的两部分，那么两个子集群都无法达到“多数”的条件。